import asyncio
from meme_tracker.social_media import SocialMediaScraper
from meme_tracker.config.settings import RedditConfig
import logging

async def test_reddit_integration():
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # éªŒè¯é…ç½®
    try:
        RedditConfig.validate()
        logging.info("é…ç½®éªŒè¯é€šè¿‡")
    except Exception as e:
        logging.error(f"é…ç½®é”™è¯¯: {e}")
        return False

    # æµ‹è¯•Reddité›†æˆ
    scraper = SocialMediaScraper()
    try:
        logging.info("æµ‹è¯•Redditè®¤è¯...")
        await scraper.reddit.authenticate()
        logging.info("âœ“ è®¤è¯æˆåŠŸ")
        
        logging.info("æµ‹è¯•å¸–å­è·å–...")
        posts = await scraper.get_reddit_posts(limit=3)
        logging.info(f"âœ“ è·å–åˆ°{len(posts)}æ¡å¸–å­")
        
        logging.info("æµ‹è¯•Unicodeå¤„ç†...")
        sample = posts[0] if posts else 'æµ‹è¯•å†…å®¹ ğŸš€ğŸ”¥ ä½ å¥½'
        processed = sample.encode('utf-8', errors='replace').decode('utf-8')
        logging.info(f"åŸå§‹: {sample[:100]}...")
        logging.info(f"å¤„ç†å: {processed[:100]}...")
        
        return True
    except Exception as e:
        logging.error(f"æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return False
    finally:
        await scraper.close()

if __name__ == "__main__":
    result = asyncio.run(test_reddit_integration())
    print("\n=== æµ‹è¯•ç»“æœ ===")
    print("çŠ¶æ€:", "é€šè¿‡ âœ…" if result else "å¤±è´¥ âŒ")