import asyncio
from meme_tracker.social_media import SocialMediaScraper
from meme_tracker.config.settings import RedditConfig
import logging

async def validate_system():
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # éªŒè¯é…ç½®
    try:
        RedditConfig.validate()
        logging.info("é…ç½®éªŒè¯é€šè¿‡")
    except Exception as e:
        logging.error(f"é…ç½®é”™è¯¯: {e}")
        return False

    # æµ‹è¯•Reddité›†æˆ
    scraper = SocialMediaScraper()
    try:
        logging.info("æµ‹è¯•Redditè®¤è¯...")
        await scraper.reddit.authenticate()
        logging.info("âœ“ è®¤è¯æˆåŠŸ")
        
        logging.info("æµ‹è¯•å¸–å­è·å–...")
        posts = await scraper.get_reddit_posts(limit=3)
        logging.info(f"âœ“ è·å–åˆ°{len(posts)}æ¡å¸–å­")
        
        logging.info("æµ‹è¯•Unicodeå¤„ç†...")
        sample = posts[0] if posts else 'æµ‹è¯•å†…å®¹ ğŸš€ğŸ”¥ ä½ å¥½'
        processed = sample.encode('utf-8', errors='replace').decode('utf-8')
        logging.info(f"åŸå§‹: {sample[:100]}...")
        logging.info(f"å¤„ç†å: {processed[:100]}...")
        
        return True
    except Exception as e:
        logging.error(f"æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return False
    finally:
        await scraper.close()

if __name__ == "__main__":
    result = asyncio.run(validate_system())
    print("\n=== éªŒè¯ç»“æœ ===")
    print("çŠ¶æ€:", "é€šè¿‡ âœ…" if result else "å¤±è´¥ âŒ")
    if not result:
        print("\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ£€æŸ¥:")
        print("1. ç¡®è®¤Redditåº”ç”¨ç±»å‹ä¸º'è„šæœ¬(script)'")
        print("2. åœ¨Redditè´¦å·è®¾ç½®ä¸­å¯ç”¨'å…è®¸å¯†ç ç™»å½•'")
        print("3. åœ¨Redditåº”ç”¨è®¾ç½®ä¸­å¯ç”¨'å¯†ç æˆæƒ'é€‰é¡¹")
        print("4. ç¡®è®¤.envæ–‡ä»¶ä¸­çš„å‡­è¯å®Œå…¨æ­£ç¡®")